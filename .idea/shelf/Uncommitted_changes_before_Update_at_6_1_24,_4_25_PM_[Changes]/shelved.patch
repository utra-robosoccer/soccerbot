Index: soccer_perception/soccer_object_localization/src/soccer_object_localization/detector_fieldline.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>#!/usr/bin/env python3\nimport os\nimport time\n\nfrom rospy.impl.tcpros_base import DEFAULT_BUFF_SIZE\nfrom tf import TransformBroadcaster\n\nfrom soccer_common.transformation import Transformation\nfrom soccer_msgs.msg import RobotState\n\nif \"ROS_NAMESPACE\" not in os.environ:\n    os.environ[\"ROS_NAMESPACE\"] = \"/robot1\"\nimport cv2\nimport numpy as np\nimport rospy\nimport sensor_msgs.point_cloud2 as pcl2\nfrom cv_bridge import CvBridge\nfrom geometry_msgs.msg import PoseWithCovarianceStamped\nfrom sensor_msgs.msg import Image, PointCloud2\nfrom soccer_object_localization.detector import Detector\nfrom std_msgs.msg import Bool, Header\n\n\nclass DetectorFieldline(Detector):\n    def __init__(self):\n        super().__init__()\n\n        self.initial_pose_subscriber = rospy.Subscriber(\"initialpose\", PoseWithCovarianceStamped, self.initial_pose_callback, queue_size=1)\n        self.image_subscriber = rospy.Subscriber(\n            \"camera/image_raw\", Image, self.image_callback, queue_size=1, buff_size=DEFAULT_BUFF_SIZE * 64\n        )  # Large buff size (https://answers.ros.org/question/220502/image-subscriber-lag-despite-queue-1/)\n        # self.image_publisher = rospy.Publisher(\"camera/line_image\", Image, queue_size=1)\n        self.point_cloud_publisher = rospy.Publisher(\"field_point_cloud\", PointCloud2, queue_size=1)\n        self.tf_broadcaster = TransformBroadcaster()\n\n        self.point_cloud_max_distance = rospy.get_param(\"point_cloud_max_distance\", 5)\n        self.point_cloud_spacing = rospy.get_param(\"point_cloud_spacing\", 30)\n        self.publish_point_cloud = False\n        self.ground_truth = False\n\n        cv2.setRNGSeed(12345)\n        pass\n\n    def initial_pose_callback(self, initial_pose: PoseWithCovarianceStamped):\n        self.publish_point_cloud = True\n\n    def image_callback(self, image: Image, debug=False):\n\n        t_start = time.time()\n\n        if self.robot_state.status not in [\n            RobotState.STATUS_READY,\n            RobotState.STATUS_LOCALIZING,\n            RobotState.STATUS_WALKING,\n            RobotState.STATUS_DETERMINING_SIDE,\n        ]:\n            return\n\n        if not self.camera.ready():\n            return\n\n        timestamp = 0  # placeholder, instead of img.header.stamp from img_msg\n        if self.ground_truth:\n            if not self.publish_point_cloud:\n                self.camera.reset_position(timestamp=timestamp)\n            else:\n                self.camera.reset_position(timestamp=timestamp, from_world_frame=True, camera_frame=\"/camera_gt\")\n        else:\n            self.camera.reset_position(timestamp=timestamp)\n        # Uncomment for ground truth\n        # rospy.loginfo_once(\"Started Publishing Fieldlines\")\n\n        # image = CvBridge().imgmsg_to_cv2(img, desired_encoding=\"rgb8\")\n        h = self.camera.calculateHorizonCoverArea()\n        if h + 1 >= self.camera.resolution_y:\n            return\n        image_crop = image[h + 1 :, :, :]\n        # image_crop_blurred = cv2.GaussianBlur(image_crop, (3, 3), 0)\n        image_crop_blurred = cv2.bilateralFilter(image_crop, 9, 75, 75)\n\n        hsv = cv2.cvtColor(src=image_crop_blurred, code=cv2.COLOR_BGR2HSV)\n\n        if debug:\n            cv2.imshow(\"CVT Color\", image_crop)\n            cv2.imshow(\"CVT Color Contrast\", image_crop_blurred)\n            cv2.waitKey(0)\n\n        # Grass Mask\n        # Hue > 115 needed\n        grass_only = cv2.inRange(hsv, (35, 85, 0), (115, 255, 255))\n        grass_only = cv2.vconcat([np.zeros((h + 1, grass_only.shape[1]), dtype=grass_only.dtype), grass_only])\n\n        # Use odd numbers for all circular masks otherwise the line will shift location\n        grass_only_0 = cv2.morphologyEx(grass_only, cv2.MORPH_OPEN, self.circular_mask(5))\n        grass_only_1 = cv2.morphologyEx(grass_only, cv2.MORPH_CLOSE, self.circular_mask(5))\n        grass_only_2 = cv2.morphologyEx(grass_only_1, cv2.MORPH_OPEN, self.circular_mask(21))\n        grass_only_3 = cv2.morphologyEx(grass_only_2, cv2.MORPH_CLOSE, self.circular_mask(61))\n\n        grass_only_morph = cv2.morphologyEx(grass_only_3, cv2.MORPH_ERODE, self.circular_mask(9))\n        grass_only_flipped = cv2.bitwise_not(grass_only)\n\n        lines_only = cv2.bitwise_and(grass_only_flipped, grass_only_flipped, mask=grass_only_morph)\n        lines_only = cv2.morphologyEx(lines_only, cv2.MORPH_CLOSE, self.circular_mask(5))\n\n        if debug:\n            cv2.imshow(\"grass_only\", grass_only)\n            cv2.imwrite(\"/tmp/grass_only.png\", grass_only)\n            cv2.imshow(\"grass_only_0\", grass_only_0)\n            cv2.imwrite(\"/tmp/grass_only_0.png\", grass_only_0)\n            cv2.imshow(\"grass_only_1\", grass_only_1)\n            cv2.imwrite(\"/tmp/grass_only_1.png\", grass_only_1)\n            cv2.imshow(\"grass_only_2\", grass_only_2)\n            cv2.imwrite(\"/tmp/grass_only_2.png\", grass_only_2)\n            cv2.imshow(\"grass_only_3\", grass_only_3)\n            cv2.imwrite(\"/tmp/grass_only_3.png\", grass_only_3)\n            cv2.imshow(\"grass_only_morph\", grass_only_morph)\n            cv2.imwrite(\"/tmp/grass_only_morph.png\", grass_only_morph)\n            cv2.imshow(\"grass_only_flipped\", grass_only_flipped)\n            cv2.imwrite(\"/tmp/grass_only_flipped.png\", grass_only_flipped)\n            cv2.imshow(\"lines_only\", lines_only)\n            cv2.imwrite(\"/tmp/lines_only.png\", lines_only)\n            cv2.waitKey(0)\n\n        # No line detection simply publish all white points\n        pts_x, pts_y = np.where(lines_only == 255)\n\n        # process images with cv2\n        cv2.imshow(\"After\", lines_only)\n\n        # ROS\n        # if self.image_publisher.get_num_connections() > 0:\n        # the following is ros (commented out)\n        # img_out = CvBridge().cv2_to_imgmsg(lines_only)\n        # img_out.header = {} # img.header\n        # self.image_publisher.publish(img_out)\n\n        # does the job of the above but using cv2 instead of ROS\n        # cv2.imshow('After', lines_only)\n\n        if self.publish_point_cloud:  # ROS and self.point_cloud_publisher.get_num_connections() > 0:\n            points3d = []\n\n            i = 0\n            for px, py in zip(pts_y, pts_x):\n                i = i + 1\n                if i % self.point_cloud_spacing == 0:\n                    camToPoint = Transformation(self.camera.findFloorCoordinate([px, py]))\n\n                    # Exclude points too far away\n                    if camToPoint.norm_squared < self.point_cloud_max_distance**2:\n                        points3d.append(camToPoint.position)\n\n            # Publish straight base link\n            self.tf_broadcaster.sendTransform(\n                self.camera.pose_base_link_straight.position,\n                self.camera.pose_base_link_straight.quaternion,\n                # img.header.stamp,\n                self.robot_name + \"/base_footprint_straight\",\n                self.robot_name + \"/odom\",\n            )\n\n            # Publish fieldlines in laserscan format\n            header = Header()\n            # header.stamp = img.header.stamp\n            header.frame_id = self.robot_name + \"/base_footprint_straight\"\n            if self.ground_truth:\n                if not self.publish_point_cloud:\n                    header.frame_id = self.robot_name + \"/base_footprint_straight\"\n                else:\n                    header.frame_id = \"world\"\n            point_cloud_msg = pcl2.create_cloud_xyz32(header, points3d)\n            self.point_cloud_publisher.publish(point_cloud_msg)\n\n        t_end = time.time()\n        rospy.loginfo_throttle(60, \"Fieldline detection rate: \" + str(t_end - t_start))\n\n\nif __name__ == \"__main__\":\n    rospy.init_node(\"detector_fieldline\")\n    fieldline_detector = DetectorFieldline()\n    rospy.spin()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/soccer_perception/soccer_object_localization/src/soccer_object_localization/detector_fieldline.py b/soccer_perception/soccer_object_localization/src/soccer_object_localization/detector_fieldline.py
--- a/soccer_perception/soccer_object_localization/src/soccer_object_localization/detector_fieldline.py	(revision 941a774aca1bef0fe26ee3cd7f4669a7aa38378a)
+++ b/soccer_perception/soccer_object_localization/src/soccer_object_localization/detector_fieldline.py	(date 1716666123574)
@@ -12,7 +12,7 @@
     os.environ["ROS_NAMESPACE"] = "/robot1"
 import cv2
 import numpy as np
-import rospy
+# import rospy
 import sensor_msgs.point_cloud2 as pcl2
 from cv_bridge import CvBridge
 from geometry_msgs.msg import PoseWithCovarianceStamped
@@ -25,16 +25,16 @@
     def __init__(self):
         super().__init__()
 
-        self.initial_pose_subscriber = rospy.Subscriber("initialpose", PoseWithCovarianceStamped, self.initial_pose_callback, queue_size=1)
-        self.image_subscriber = rospy.Subscriber(
-            "camera/image_raw", Image, self.image_callback, queue_size=1, buff_size=DEFAULT_BUFF_SIZE * 64
-        )  # Large buff size (https://answers.ros.org/question/220502/image-subscriber-lag-despite-queue-1/)
-        # self.image_publisher = rospy.Publisher("camera/line_image", Image, queue_size=1)
-        self.point_cloud_publisher = rospy.Publisher("field_point_cloud", PointCloud2, queue_size=1)
-        self.tf_broadcaster = TransformBroadcaster()
-
-        self.point_cloud_max_distance = rospy.get_param("point_cloud_max_distance", 5)
-        self.point_cloud_spacing = rospy.get_param("point_cloud_spacing", 30)
+        # self.initial_pose_subscriber = rospy.Subscriber("initialpose", PoseWithCovarianceStamped, self.initial_pose_callback, queue_size=1)
+        # self.image_subscriber = rospy.Subscriber(
+        #     "camera/image_raw", Image, self.image_callback, queue_size=1, buff_size=DEFAULT_BUFF_SIZE * 64
+        # )  # Large buff size (https://answers.ros.org/question/220502/image-subscriber-lag-despite-queue-1/)
+        # # self.image_publisher = rospy.Publisher("camera/line_image", Image, queue_size=1)
+        # self.point_cloud_publisher = rospy.Publisher("field_point_cloud", PointCloud2, queue_size=1)
+        # self.tf_broadcaster = TransformBroadcaster()
+        #
+        # self.point_cloud_max_distance = rospy.get_param("point_cloud_max_distance", 5)
+        # self.point_cloud_spacing = rospy.get_param("point_cloud_spacing", 30)
         self.publish_point_cloud = False
         self.ground_truth = False
 
@@ -172,10 +172,10 @@
             self.point_cloud_publisher.publish(point_cloud_msg)
 
         t_end = time.time()
-        rospy.loginfo_throttle(60, "Fieldline detection rate: " + str(t_end - t_start))
+        # rospy.loginfo_throttle(60, "Fieldline detection rate: " + str(t_end - t_start))
 
 
-if __name__ == "__main__":
-    rospy.init_node("detector_fieldline")
-    fieldline_detector = DetectorFieldline()
-    rospy.spin()
+# if __name__ == "__main__":
+    # rospy.init_node("detector_fieldline")
+    # fieldline_detector = DetectorFieldline()
+    # rospy.spin()
Index: soccer_common/src/soccer_common/utils_rosparam.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from os import path\nfrom unittest.mock import MagicMock\n\nimport rosparam\nimport rospy\nimport yaml\n\n\ndef set_rosparam_from_yaml_file(param_path: str = \"\", delete_params=True, convert_logs_to_prints=True):\n    if rospy.get_node_uri() is None:\n        rospy.init_node(\"test\")\n\n    if convert_logs_to_prints:\n        rospy.set_param(\"/use_sim_time\", False)\n\n        rospy.loginfo_throttle = lambda a, b: None\n        rospy.loginfo = lambda a: print(a)\n        rospy.logwarn = lambda a: print(a)\n        rospy.logerr = lambda a: print(a)\n\n    if delete_params:\n        rosparam.delete_param(\"/\")\n    if path.exists(param_path):\n        with open(param_path) as f:\n            param_info = yaml.safe_load(f)\n            rosparam.upload_params(rospy.get_namespace(), param_info)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/soccer_common/src/soccer_common/utils_rosparam.py b/soccer_common/src/soccer_common/utils_rosparam.py
--- a/soccer_common/src/soccer_common/utils_rosparam.py	(revision 941a774aca1bef0fe26ee3cd7f4669a7aa38378a)
+++ b/soccer_common/src/soccer_common/utils_rosparam.py	(date 1716665324788)
@@ -10,17 +10,17 @@
     if rospy.get_node_uri() is None:
         rospy.init_node("test")
 
-    if convert_logs_to_prints:
-        rospy.set_param("/use_sim_time", False)
-
-        rospy.loginfo_throttle = lambda a, b: None
-        rospy.loginfo = lambda a: print(a)
-        rospy.logwarn = lambda a: print(a)
-        rospy.logerr = lambda a: print(a)
-
-    if delete_params:
-        rosparam.delete_param("/")
-    if path.exists(param_path):
-        with open(param_path) as f:
-            param_info = yaml.safe_load(f)
-            rosparam.upload_params(rospy.get_namespace(), param_info)
+    # if convert_logs_to_prints:
+    #     rospy.set_param("/use_sim_time", False)
+    #
+    #     rospy.loginfo_throttle = lambda a, b: None
+    #     rospy.loginfo = lambda a: print(a)
+    #     rospy.logwarn = lambda a: print(a)
+    #     rospy.logerr = lambda a: print(a)
+    #
+    # if delete_params:
+    #     rosparam.delete_param("/")
+    # if path.exists(param_path):
+    #     with open(param_path) as f:
+    #         param_info = yaml.safe_load(f)
+    #         rosparam.upload_params(rospy.get_namespace(), param_info)
Index: soccer_perception/soccer_object_localization/src/soccer_object_localization/test_object_localization.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import os\n\nfrom soccer_object_detection.object_detect_node import Label, ObjectDetectionNode\nfrom soccer_object_detection.test_object_detection import IoU\nfrom soccer_object_localization.detector_objects import DetectorObjects\n\nos.environ[\"ROS_NAMESPACE\"] = \"/robot1\"\n\nimport math\nfrom unittest import TestCase\nfrom unittest.mock import MagicMock\n\nimport cv2\nimport numpy as np\nimport pytest\nimport rosbag\nimport rospy\nimport tf2_ros\nfrom cv2 import Mat\nfrom cv_bridge import CvBridge\nfrom sensor_msgs.msg import CameraInfo, Image\nfrom soccer_object_localization.detector_fieldline import DetectorFieldline\nfrom soccer_object_localization.detector_goalpost import DetectorGoalPost\n\nfrom soccer_common.camera import Camera\nfrom soccer_common.transformation import Transformation\nfrom soccer_common.utils import download_dataset, wrapToPi\nfrom soccer_msgs.msg import GameState, RobotState\n\n\nclass TestObjectLocalization(TestCase):\n    def test_camera_find_floor_coordinate(self):\n        rospy.init_node(\"test\")\n        p = Transformation([0, 0, 0.5], euler=[0, math.pi / 4, 0])\n        c = Camera(\"robot1\")\n        c.pose = p\n        ci = CameraInfo()\n        ci.height = 240\n        ci.width = 360\n        c.camera_info = ci\n\n        p2 = c.findFloorCoordinate([360 / 2, 240 / 2])\n        self.assertAlmostEqual(p2[0], 0.5, delta=0.005)\n        self.assertAlmostEqual(p2[1], 0, delta=0.005)\n        self.assertAlmostEqual(p2[2], 0, delta=0.005)\n\n    def test_camera_find_camera_coordinate(self):\n        rospy.init_node(\"test\")\n        p = Transformation([0, 0, 0.5], euler=[0, math.pi / 4, 0])\n        c = Camera(\"robot1\")\n        c.pose = p\n        ci = CameraInfo()\n        ci.height = 240\n        ci.width = 360\n        c.camera_info = ci\n\n        p2 = c.findCameraCoordinate([0.5, 0, 0])\n        self.assertAlmostEqual(p2[0], 360 / 2, delta=0.5)\n        self.assertAlmostEqual(p2[1], 240 / 2, delta=0.5)\n\n    def test_camera_find_camera_coordinate_2(self):\n        rospy.init_node(\"test\")\n        p = Transformation([0, 0, 0.5], euler=[0, 0, 0])\n        c = Camera(\"robot1\")\n        c.pose = p\n        ci = CameraInfo()\n        ci.height = 240\n        ci.width = 360\n        c.camera_info = ci\n\n        p3 = c.findCameraCoordinate([0.5, 0, 0.5])\n        self.assertAlmostEqual(p3[0], 360 / 2, delta=0.5)\n        self.assertAlmostEqual(p3[1], 240 / 2, delta=0.5)\n\n    def test_calculate_bounding_boxes_from_ball(self):\n        rospy.init_node(\"test\")\n\n        for cam_angle in [0, 0.1, -0.1]:\n            for cam_position in [[0, 0, 0], [0, 0, 0.1], [0, 0, -0.1]]:\n                p = Transformation(cam_position, euler=[cam_angle, 0, 0])\n                c = Camera(\"robot1\")\n                c.pose = p\n                ci = CameraInfo()\n                ci.height = 240\n                ci.width = 360\n                c.camera_info = ci\n\n                positions = [[0.5, 0, 0.1], [0.5, 0, 0], [0.5, 0, 0.1]]\n                for position in positions:\n                    ball_pose = Transformation(position)\n                    ball_radius = 0.07\n\n                    bounding_boxes = c.calculateBoundingBoxesFromBall(ball_pose, ball_radius)\n                    # [[135.87634651355825, 75.87634651355823], [224.12365348644175, 164.12365348644175]]\n                    position = c.calculateBallFromBoundingBoxes(ball_radius, bounding_boxes)\n\n                    self.assertAlmostEqual(position.position[0], ball_pose.position[0], delta=0.001)\n                    self.assertAlmostEqual(position.position[1], ball_pose.position[1], delta=0.001)\n                    self.assertAlmostEqual(position.position[2], ball_pose.position[2], delta=0.001)\n\n    def test_fieldline_detection(self):\n        # rospy.init_node(\"test\")\n\n        # remove ros and magic mock will be handled later\n        # magic mock provides dummy variables such that an instantiated function does not crash\n\n        src_path = os.path.dirname(os.path.realpath(__file__))\n        test_path = src_path + \"/../../images/fieldlines\"\n        download_dataset(url=\"https://drive.google.com/uc?id=1nJX6ySks_a7mESvCm3sNllmJTNpm-x2_\", folder_path=test_path)\n\n        Camera.reset_position = MagicMock()\n        Camera.ready = MagicMock()\n        d = DetectorFieldline()\n        d.robot_state.status = RobotState.STATUS_READY\n        # ROS\n        # d.image_publisher.get_num_connections = MagicMock(return_value=1)\n        # d.publish_point_cloud = True\n        # d.point_cloud_publisher.get_num_connections = MagicMock(return_value=1)\n\n        cvbridge = CvBridge()\n\n        for file_name in os.listdir(test_path):\n            # file_name = \"img160_-1.452993567956688_-3.15_0.7763055830612666.png\"\n\n            print(file_name)\n            img: Mat = cv2.imread(os.path.join(test_path, file_name))\n\n            c = CameraInfo()\n            c.height = img.shape[0]\n            c.width = img.shape[1]\n            d.camera.camera_info = c\n\n            # ROS\n            # img_msg: Image = cvbridge.cv2_to_imgmsg(img, encoding=\"rgb8\")\n            # d.image_publisher.publish = MagicMock()\n            d.image_callback(img, debug=False)\n\n            if \"DISPLAY\" in os.environ:\n                cv2.imshow(\"Before\", img)\n                cv2.imwrite(\"/tmp/before.png\", img)\n\n                # ROS\n                # if d.image_publisher.publish.call_count != 0:\n                #     img_out = cvbridge.imgmsg_to_cv2(d.image_publisher.publish.call_args[0][0])\n                #     cv2.imshow(\"After\", img_out)\n                #     cv2.imwrite(\"/tmp/after.png\", img_out)\n\n                cv2.waitKey(0)\n        cv2.destroyAllWindows()\n\n    def test_fieldline_detection_freehicle(self):\n        rospy.init_node(\"test\")\n        rospy.set_param(\"point_cloud_max_distance\", 20)\n        rospy.set_param(\"point_cloud_spacing\", 20)\n\n        src_path = os.path.dirname(os.path.realpath(__file__))\n        folter_path = src_path + \"/../../images/freehicle\"\n        download_dataset(url=\"https://drive.google.com/uc?id=1Df7FMbnvJ5d7jAx-8fPiaFiLcd5nrsT9\", folder_path=folter_path)\n\n        test_path = folter_path + \"/freehicle.bag\"\n        test_out = folter_path + \"/freehicle_out.bag\"\n\n        bag = rosbag.Bag(test_path)\n        bag_out = rosbag.Bag(test_out, mode=\"w\")\n        Camera.reset_position = MagicMock()\n        Camera.ready = MagicMock()\n        d = DetectorFieldline()\n        d.camera.horizontalFOV = 1.3926 * 800 / 640\n        d.robot_name = \"robot1\"\n        d.robot_state.status = RobotState.STATUS_READY\n        d.image_publisher.get_num_connections = MagicMock(return_value=1)\n        d.point_cloud_publisher.get_num_connections = MagicMock(return_value=1)\n        d.publish_point_cloud = True\n\n        tfBuffer = tf2_ros.Buffer(rospy.Duration(1000))\n        tl = tf2_ros.TransformListener(tfBuffer)\n        camera_detected = False\n        debug = False\n        cvbridge = CvBridge()\n        original_publish = d.point_cloud_publisher.publish\n        t_init = None\n        for topic, msg, t in bag.read_messages():\n            if t_init is None:\n                t_init = t\n            else:\n                if t.secs - t_init.secs > 10:\n                    break\n            if topic == \"/odom\":\n                topic = \"/robot1/odom_combined\"\n            bag_out.write(topic, msg, t)\n\n            if topic == \"/camera/image_raw\":\n                if not camera_detected:\n                    continue\n\n                c = CameraInfo()\n                c.height = msg.height\n                c.width = msg.width\n                d.camera.camera_info = c\n\n                tf_stamped = tfBuffer.lookup_transform(\"map\", \"camera\", rospy.Time())\n                d.camera.pose.geometry_msgs_transform = tf_stamped.transform\n                position_original = d.camera.pose.position\n                orientation_euler_original = d.camera.pose.orientation_euler\n                position_original[0:2] = 0\n                orientation_euler_original[0] = 0\n                orientation_euler_original[2] = 0\n                d.camera.pose.position = position_original\n                d.camera.pose.orientation_euler = orientation_euler_original\n                img = cvbridge.imgmsg_to_cv2(msg)\n                d.image_publisher.publish = MagicMock()\n\n                d.point_cloud_publisher.publish = MagicMock()\n                d.image_callback(msg, debug=debug)\n\n                bag_out.write(\"/robot1/field_point_cloud\", d.point_cloud_publisher.publish.call_args[0][0], t)\n                original_publish(d.point_cloud_publisher.publish.call_args[0][0])\n\n                if \"DISPLAY\" in os.environ:\n                    cv2.imshow(\"Before\", img)\n\n                    if d.image_publisher.publish.call_count != 0:\n                        img_out = cvbridge.imgmsg_to_cv2(d.image_publisher.publish.call_args[0][0])\n                        cv2.imshow(\"After\", img_out)\n                        cv2.waitKey(1)\n\n            elif topic == \"/tf\":\n                camera_detected = True\n                msg._connection_header = MagicMock()\n                msg._connection_header.get = MagicMock(return_value=\"default_authority\")\n                tl.callback(msg)\n            elif topic == \"/tf_static\":\n                msg._connection_header = MagicMock()\n                msg._connection_header.get = MagicMock(return_value=\"default_authority\")\n                tl.static_callback(msg)\n                pass\n\n        bag_out.close()\n        bag.close()\n        if \"DISPLAY\" in os.environ:\n            cv2.destroyAllWindows()\n\n    def test_goalpost_detection(self):\n        rospy.init_node(\"test\")\n\n        \"\"\"\n            Returns whether a point at a given field coordinate is visible to the robot\n        \"\"\"\n\n        def get_point_visibility(robot_pose, point_coords):\n            robot_x, robot_y, robot_yaw = robot_pose\n            point_x, point_y = point_coords\n\n            point_yaw = math.atan2(point_y - robot_y, point_x - robot_x)\n            camera_fov = 1.39626  # rads\n\n            # Both yaw angles are between -pi and pi\n            delta_yaw = wrapToPi(point_yaw - robot_yaw)\n\n            # Check if the point is within the view cone\n            # No equals case as the point wouldn't be fully visible\n            is_point_visible = -camera_fov / 2.0 < delta_yaw < camera_fov / 2.0\n\n            return is_point_visible\n\n        \"\"\"\n            Returns a dictionary that stores booleans indicating whether each post is visible\n            Visual reference: https://www.desmos.com/calculator/b9lndsb1bl\n            Example: both posts of the left net are visible\n            visible_posts = {\n                \"NEG_X_NET\": {\n                    \"POS_Y_POST\": True,\n                    \"NEG_Y_POST\": True\n                },\n                \"POS_X_NET\": {\n                    \"POS_Y_POST\": False,\n                    \"NEG_Y_POST\": False\n                }\n            }\n        \"\"\"\n\n        def get_visible_posts(robot_x, robot_y, robot_yaw):\n            visible_posts = {\"NEG_X_NET\": {\"POS_Y_POST\": True, \"NEG_Y_POST\": True}, \"POS_X_NET\": {\"POS_Y_POST\": False, \"NEG_Y_POST\": False}}\n\n            net_coords = {\n                \"NEG_X_NET\": {\"POS_Y_POST\": [-4.5, 1.3], \"NEG_Y_POST\": [-4.5, -1.3]},\n                \"POS_X_NET\": {\"POS_Y_POST\": [4.5, 1.3], \"NEG_Y_POST\": [4.5, -1.3]},\n            }\n\n            for net in net_coords.keys():\n                post_coords = net_coords[net]\n                for post in post_coords.keys():\n                    visible_posts[net][post] = get_point_visibility((robot_x, robot_y, robot_yaw), net_coords[net][post])\n\n            return visible_posts\n\n        # Setup test environment:\n        src_path = os.path.dirname(os.path.realpath(__file__))\n        test_path = src_path + \"/../../images/goal_net\"\n        download_dataset(url=\"https://drive.google.com/uc?id=17qdnW7egoopXHvakiNnUUufP2MOjyZ18\", folder_path=test_path)\n\n        Camera.reset_position = MagicMock()\n        Camera.ready = MagicMock()\n        d = DetectorGoalPost()\n        d.robot_state.status = RobotState.STATUS_DETERMINING_SIDE\n        d.camera.pose = Transformation(position=[0, 0, 0.46])\n        d.image_publisher.get_num_connections = MagicMock(return_value=1)\n\n        cvbridge = CvBridge()\n\n        src_path = os.path.dirname(os.path.realpath(__file__))\n        test_path = src_path + \"/../../images/goal_net\"\n\n        # Loop through test images\n        for file_name in os.listdir(test_path):\n            # file_name = \"img173_-0.852141317992289_3.15_-1.7125376246657054.png\"\n\n            print(f\"Loading {file_name} from goal_net dataset\")\n            file_name_no_ext = os.path.splitext(file_name)[0]\n            x, y, yaw = file_name_no_ext.split(\"_\")[1:]\n            yaw = wrapToPi(float(yaw))\n            if yaw < 0:\n                yaw = (yaw + np.pi) % (np.pi)\n\n            d.camera.pose.orientation_euler = [yaw, 0, 0]\n            print(f\"Parsed (x, y, yaw): ({x}, {y}, {yaw}) from filename.\")\n            visible_posts = get_visible_posts(float(x), float(y), float(yaw))\n            for net in visible_posts.keys():\n                for post in visible_posts[net].keys():\n                    if visible_posts[net][post]:\n                        print(f\"{net}, {post} is visible\")\n\n            img: Mat = cv2.imread(os.path.join(test_path, file_name))\n\n            if \"DISPLAY\" in os.environ:\n                cv2.imshow(\"Before\", img)\n                cv2.waitKey(0)\n\n            c = CameraInfo()\n            c.height = img.shape[0]\n            c.width = img.shape[1]\n            d.camera.camera_info = c\n\n            img_msg: Image = cvbridge.cv2_to_imgmsg(img, encoding=\"rgb8\")\n            d.image_publisher.publish = MagicMock()\n            d.image_callback(img_msg, debug=True)\n\n            if \"DISPLAY\" in os.environ:\n                if d.image_publisher.publish.call_count != 0:\n                    img_out = cvbridge.imgmsg_to_cv2(d.image_publisher.publish.call_args[0][0])\n                    cv2.imshow(\"After\", img_out)\n\n                cv2.waitKey(0)\n\n    @pytest.mark.skip\n    def test_goalpost_detection_tune(self):\n        \"\"\"\n        Used for tuning vertical line detection parameters using sliders.\n        \"\"\"\n\n        rospy.init_node(\"test\")\n\n        src_path = os.path.dirname(os.path.realpath(__file__))\n        test_path = src_path + \"/../../images/goal_net\"\n        download_dataset(url=\"https://drive.google.com/uc?id=17qdnW7egoopXHvakiNnUUufP2MOjyZ18\", folder_path=test_path)\n\n        Camera.reset_position = MagicMock()\n        Camera.ready = MagicMock()\n        d = DetectorGoalPost()\n\n        src_path = os.path.dirname(os.path.realpath(__file__))\n        test_path = src_path + \"/../../images/goal_net\"\n        file_name = \"img341_0.960470105738711_-3.15_1.3585673890175494.png\"\n\n        print(f\"Loading {file_name} from goal_net dataset\")\n        file_name_no_ext = os.path.splitext(file_name)[0]\n        x, y, yaw = file_name_no_ext.split(\"_\")[1:]\n        print(f\"Parsed (x, y, yaw): ({x}, {y}, {yaw}) from filename.\")\n\n        img: Mat = cv2.imread(os.path.join(test_path, file_name))\n\n        c = CameraInfo()\n        c.height = img.shape[0]\n        c.width = img.shape[1]\n        d.camera.camera_info = c\n\n        # Create a window\n        cv2.namedWindow(\"image\")\n\n        def nothing(x):\n            pass\n\n        # create trackbars for hough line parameters\n        default_vline_angle_tol_deg = 3\n        default_theta = np.pi / 180\n        default_threshold = 30\n        default_min_line_length = 30\n        default_max_line_gap = 10\n        cv2.createTrackbar(\"vLineAngleTolDeg\", \"image\", 0, 15, nothing)\n        cv2.createTrackbar(\"threshold\", \"image\", 0, 255, nothing)\n        cv2.createTrackbar(\"minLineLength\", \"image\", 0, 250, nothing)\n        cv2.createTrackbar(\"maxLineGap\", \"image\", 0, 20, nothing)\n\n        # Set default value for MAX HSV trackbars.\n        cv2.setTrackbarPos(\"vLineAngleTolDeg\", \"image\", default_vline_angle_tol_deg)\n        cv2.setTrackbarPos(\"threshold\", \"image\", default_threshold)\n        cv2.setTrackbarPos(\"minLineLength\", \"image\", default_min_line_length)\n        cv2.setTrackbarPos(\"maxLineGap\", \"image\", default_max_line_gap)\n\n        while True:\n            # get current positions of all trackbars\n            vline_angle_tol_deg = cv2.getTrackbarPos(\"vLineAngleTolDeg\", \"image\")\n            threshold = cv2.getTrackbarPos(\"threshold\", \"image\")\n            min_line_length = cv2.getTrackbarPos(\"minLineLength\", \"image\")\n            max_line_gap = cv2.getTrackbarPos(\"maxLineGap\", \"image\")\n\n            vertical_lines, img_out = d.get_vlines_from_img(\n                img,\n                debug=False,\n                angle_tol_deg=vline_angle_tol_deg,\n                hough_theta=default_theta,\n                hough_threshold=threshold,\n                hough_min_line_length=min_line_length,\n                hough_max_line_gap=max_line_gap,\n            )\n            cv2.imshow(\"image\", img_out)\n\n            if cv2.waitKey(33) & 0xFF == ord(\"n\"):\n                file_name = \"newfile\"\n                break\n\n    @pytest.mark.skip\n    def test_hsv_filter(self):\n        import cv2\n        import numpy as np\n\n        def nothing(x):\n            pass\n\n        # Create a window\n        cv2.namedWindow(\"image\")\n\n        # create trackbars for color change\n        cv2.createTrackbar(\"HMin\", \"image\", 0, 179, nothing)  # Hue is from 0-179 for Opencv\n        cv2.createTrackbar(\"SMin\", \"image\", 0, 255, nothing)\n        cv2.createTrackbar(\"VMin\", \"image\", 0, 255, nothing)\n        cv2.createTrackbar(\"HMax\", \"image\", 0, 179, nothing)\n        cv2.createTrackbar(\"SMax\", \"image\", 0, 255, nothing)\n        cv2.createTrackbar(\"VMax\", \"image\", 0, 255, nothing)\n\n        # Set default value for MAX HSV trackbars.\n        cv2.setTrackbarPos(\"HMax\", \"image\", 179)\n        cv2.setTrackbarPos(\"SMax\", \"image\", 255)\n        cv2.setTrackbarPos(\"VMax\", \"image\", 255)\n\n        # Initialize to check if HSV min/max value changes\n        hMin = sMin = vMin = hMax = sMax = vMax = 0\n        phMin = psMin = pvMin = phMax = psMax = pvMax = 0\n\n        img = cv2.imread(\"../../images/goal_net/img160_-1.452993567956688_-3.15_0.7763055830612666.png\")\n        output = img\n        waitTime = 33\n\n        while 1:\n\n            # get current positions of all trackbars\n            hMin = cv2.getTrackbarPos(\"HMin\", \"image\")\n            sMin = cv2.getTrackbarPos(\"SMin\", \"image\")\n            vMin = cv2.getTrackbarPos(\"VMin\", \"image\")\n\n            hMax = cv2.getTrackbarPos(\"HMax\", \"image\")\n            sMax = cv2.getTrackbarPos(\"SMax\", \"image\")\n            vMax = cv2.getTrackbarPos(\"VMax\", \"image\")\n\n            # Set minimum and max HSV values to display\n            lower = np.array([hMin, sMin, vMin])\n            upper = np.array([hMax, sMax, vMax])\n\n            # Create HSV Image and threshold into a range.\n            hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n            mask = cv2.inRange(hsv, lower, upper)\n            output = cv2.bitwise_and(img, img, mask=mask)\n\n            # Print if there is a change in HSV value\n            if (phMin != hMin) | (psMin != sMin) | (pvMin != vMin) | (phMax != hMax) | (psMax != sMax) | (pvMax != vMax):\n                print(\"(hMin = %d , sMin = %d, vMin = %d), (hMax = %d , sMax = %d, vMax = %d)\" % (hMin, sMin, vMin, hMax, sMax, vMax))\n                phMin = hMin\n                psMin = sMin\n                pvMin = vMin\n                phMax = hMax\n                psMax = sMax\n                pvMax = vMax\n\n            # Display output image\n            cv2.imshow(\"image\", output)\n\n            # Wait longer to prevent freeze for videos.\n            if cv2.waitKey(waitTime) & 0xFF == ord(\"q\"):\n                break\n\n        cv2.destroyAllWindows()\n\n    def test_robot_detection(self):\n        src_path = os.path.dirname(os.path.realpath(__file__))\n        test_path = src_path + \"/../../../soccer_object_detection/images/simulation\"\n        download_dataset(\"https://drive.google.com/uc?id=11nN58j8_PBoLNRAzOEdk7fMe1UK1diCc\", folder_path=test_path)\n\n        rospy.init_node(\"test\")\n\n        Camera.reset_position = MagicMock()\n\n        src_path = os.path.dirname(os.path.realpath(__file__))\n        model_path = src_path + \"/../../../soccer_object_detection/models/half_5.pt\"\n\n        n = ObjectDetectionNode(model_path=model_path)\n        n.robot_state.status = RobotState.STATUS_READY\n        n.game_state.gameState = GameState.GAMESTATE_PLAYING\n\n        Camera.reset_position = MagicMock()\n        Camera.ready = MagicMock()\n        do = DetectorObjects()\n        do.robot_state.status = RobotState.STATUS_READY\n\n        cvbridge = CvBridge()\n        for file_name in os.listdir(f\"{test_path}/images\"):\n            print(file_name)\n            img: Mat = cv2.imread(os.path.join(f\"{test_path}/images\", file_name))  # ground truth box = (68, 89) (257, 275)\n            img_original_size = img.size\n            img = cv2.resize(img, dsize=(640, 480))\n            img_msg: Image = cvbridge.cv2_to_imgmsg(img)\n\n            # Mock the detections\n            n.pub_detection = MagicMock()\n            n.pub_boundingbox = MagicMock()\n            n.pub_detection.get_num_connections = MagicMock(return_value=1)\n            n.pub_boundingbox.get_num_connections = MagicMock(return_value=1)\n            n.pub_detection.publish = MagicMock()\n            n.pub_boundingbox.publish = MagicMock()\n\n            ci = CameraInfo()\n            ci.height = img.shape[0]\n            ci.width = img.shape[1]\n            n.camera.camera_info = ci\n            do.camera.camera_info = ci\n            n.camera.pose = Transformation(position=[0, 0, 0.46], orientation_euler=[0, np.pi / 8, 0])\n            do.camera.pose = n.camera.pose\n\n            n.callback(img_msg)\n\n            # Check assertion\n            if n.pub_boundingbox.publish.call_args is not None:\n                bounding_boxes = n.pub_boundingbox.publish.call_args[0][0]\n                do.objectDetectorCallback(bounding_boxes)\n\n            if \"DISPLAY\" in os.environ:\n                mat = cvbridge.imgmsg_to_cv2(n.pub_detection.publish.call_args[0][0])\n                cv2.imshow(\"Image\", mat)\n                cv2.waitKey()\n\n        cv2.destroyAllWindows()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/soccer_perception/soccer_object_localization/src/soccer_object_localization/test_object_localization.py b/soccer_perception/soccer_object_localization/src/soccer_object_localization/test_object_localization.py
--- a/soccer_perception/soccer_object_localization/src/soccer_object_localization/test_object_localization.py	(revision 941a774aca1bef0fe26ee3cd7f4669a7aa38378a)
+++ b/soccer_perception/soccer_object_localization/src/soccer_object_localization/test_object_localization.py	(date 1716665953845)
@@ -1,7 +1,9 @@
 import os
 
 from soccer_object_detection.object_detect_node import Label, ObjectDetectionNode
-from soccer_object_detection.test_object_detection import IoU
+# from soccer_object_detection.test_object_detection import IoU
+# from soccer_common.utils_rosparam import set_rosparam_from_yaml_file
+# set_rosparam_from_yaml_file()
 from soccer_object_localization.detector_objects import DetectorObjects
 
 os.environ["ROS_NAMESPACE"] = "/robot1"
Index: external/hlvs_webots/worlds/.robocup.wbproj
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>Webots Project File version R2022b\nperspectives: 000000ff00000000fd00000003000000000000000000000000fc0100000001fb0000001a0044006f00630075006d0065006e0074006100740069006f006e0000000000ffffffff00000000000000000000000100000124000002dcfc0200000001fb0000001400540065007800740045006400690074006f00720000000015000002dc0000003c00ffffff000000030000039c0000005efc0100000001fb0000001a0043006f006e0073006f006c00650041006c006c0041006c006c01000000000000039c0000005400ffffff0000039c0000036800000001000000020000000100000008fc00000000\nsimulationViewPerspectives: 000000ff000000010000000200000157000005db0100000006010000000100\nsceneTreePerspectives: 000000ff00000001000000030000001c0000027a000000fc0100000006010000000200\nmaximizedDockId: -1\ncentralWidgetVisible: 1\northographicViewHeight: 1\ntextFiles: -1\nconsoles: Console:All:All\nrenderingDevicePerspectives: blue player 1:Camera;1;1;0;0\nrenderingDevicePerspectives: blue player 1:camera;1;1;0;0\nrenderingDevicePerspectives: blue player 2:Camera;1;1;0;0\nrenderingDevicePerspectives: blue player 2:camera;1;1;0;0\nrenderingDevicePerspectives: blue player 3:Camera;1;1;0;0\nrenderingDevicePerspectives: blue player 3:camera;1;1;0;0\nrenderingDevicePerspectives: red player 1:Camera;1;1;0;0\nrenderingDevicePerspectives: red player 1:camera;1;0.895312;0;0\nrenderingDevicePerspectives: red player 2:Camera;1;1;0;0\nrenderingDevicePerspectives: red player 2:camera;1;1;0;0\nrenderingDevicePerspectives: red player 3:Camera;1;1;0;0\nrenderingDevicePerspectives: red player 3:camera;1;1;0;0\nrenderingDevicePerspectives: red player 4:Camera;1;1;0;0\nrenderingDevicePerspectives: red player 4:camera;1;1;0;0\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/external/hlvs_webots/worlds/.robocup.wbproj b/external/hlvs_webots/worlds/.robocup.wbproj
--- a/external/hlvs_webots/worlds/.robocup.wbproj	(revision 1412194626ee7366e50cecb4ef9a1a54b44600d4)
+++ b/external/hlvs_webots/worlds/.robocup.wbproj	(date 1710013957868)
@@ -1,7 +1,7 @@
 Webots Project File version R2022b
-perspectives: 000000ff00000000fd00000003000000000000000000000000fc0100000001fb0000001a0044006f00630075006d0065006e0074006100740069006f006e0000000000ffffffff00000000000000000000000100000124000002dcfc0200000001fb0000001400540065007800740045006400690074006f00720000000015000002dc0000003c00ffffff000000030000039c0000005efc0100000001fb0000001a0043006f006e0073006f006c00650041006c006c0041006c006c01000000000000039c0000005400ffffff0000039c0000036800000001000000020000000100000008fc00000000
-simulationViewPerspectives: 000000ff000000010000000200000157000005db0100000006010000000100
-sceneTreePerspectives: 000000ff00000001000000030000001c0000027a000000fc0100000006010000000200
+perspectives: 000000ff00000000fd00000003000000000000000000000000fc0100000001fb0000001a0044006f00630075006d0065006e0074006100740069006f006e0000000000ffffffff00000000000000000000000100000124000002dcfc0200000001fb0000001400540065007800740045006400690074006f00720000000015000002dc0000003f00ffffff00000003000007380000005efc0100000001fb0000001a0043006f006e0073006f006c00650041006c006c0041006c006c0100000000000007380000006900ffffff000007380000036c00000001000000020000000100000008fc00000000
+simulationViewPerspectives: 000000ff000000010000000200000157000005db0100000002010000000100
+sceneTreePerspectives: 000000ff00000001000000030000001c0000027a000000fc0100000002010000000200
 maximizedDockId: -1
 centralWidgetVisible: 1
 orthographicViewHeight: 1
@@ -14,7 +14,7 @@
 renderingDevicePerspectives: blue player 3:Camera;1;1;0;0
 renderingDevicePerspectives: blue player 3:camera;1;1;0;0
 renderingDevicePerspectives: red player 1:Camera;1;1;0;0
-renderingDevicePerspectives: red player 1:camera;1;0.895312;0;0
+renderingDevicePerspectives: red player 1:camera;1;1;0.000665336;0
 renderingDevicePerspectives: red player 2:Camera;1;1;0;0
 renderingDevicePerspectives: red player 2:camera;1;1;0;0
 renderingDevicePerspectives: red player 3:Camera;1;1;0;0
