humanoid-ppo:
  env: gym_soccerbot:walk-forward-norm-v1
  run: PPO
  checkpoint_freq: 20
  max_failures: 10
  checkpoint_at_end: true
  local_dir: ./PPO-walk-forward-norm-v1/
  restore: /home/robosoccer/shah_ws/PPO-walk-forward-norm-v1/humanoid-ppo/PPO_gym_soccerbot:walk-forward-norm-v1_0d4b8_00000_0_2021-04-10_17-52-32/checkpoint_740/checkpoint-740
  stop:
    timesteps_total: 2000000000
  config:
    # Works for both torch and tf.
    framework: torch
    num_workers: 32
    num_cpus_per_worker: 0
    num_envs_per_worker: 4
    horizon: 2048
    num_gpus: 1 # 0.0001
    # num_gpus_per_worker: 0.03
    gamma: 0.95
    # {"OMP_NUM_THREADS": "16"}
    extra_python_environs_for_driver:
      OMP_NUM_THREADS: 32
    env_config:
      env_name: gym_soccerbot:walk-forward-v3
      # dtype: np.float32

    # Should use a critic as a baseline (otherwise don't use value baseline;
    # required for using GAE).
    use_critic: true
    # If true use the Generalized Advantage Estimator (GAE)
    # with a value function see https://arxiv.org/pdf/1506.02438.pdf.
    use_gae: true
    # The GAE (lambda) parameter.
    lambda: 0.99
    # Initial coefficient for KL divergence.
    kl_coeff: 0.2
    # Size of batches collected from each worker.
    rollout_fragment_length: 8192
    # Number of timesteps collected for each SGD round. This defines the size
    # of each SGD epoch.
    train_batch_size: 262144 #4096*32*4 # 4000
    # Total SGD batch size across all devices for SGD. This defines the
    # minibatch size within each epoch.
    sgd_minibatch_size: 8192 #4096*32*4 # 128
    # Whether to shuffle sequences in the batch when training (recommended).
    shuffle_sequences: true
    # Number of SGD iterations in each outer loop (i.e. number of epochs to
    # execute per train batch).
    num_sgd_iter: 10
    # Stepsize of SGD.
    lr: 0.001
    lr_schedule:
      null
      #    - - !!float 3e-3
      #- !!float 1e-5
      #- !!float 0
    # Learning rate schedule.
    # lr_schedule: null
    # Coefficient of the value function loss. IMPORTANT: you must tune this if
    # you set vf_share_layers=True inside your model's config.
    vf_loss_coeff: 1.0
    model:
      fcnet_hiddens: [64, 64]
      fcnet_activation: tanh
      free_log_std: false
      # Share layers for value function. If you set this to True it's
      # important to tune vf_loss_coeff.
      vf_share_layers: false
    # Coefficient of the entropy regularizer.
    entropy_coeff: 0.001
    # Decay schedule for the entropy regularizer.
    entropy_coeff_schedule: null
    #    - - !!float 1e-2
    #      - !!float 1e-3
    #      - !!float 0
    # PPO clip parameter.
    clip_param: 0.4
    # Clip param for the value function. Note that this is sensitive to the
    # scale of the rewards. If your expected V is large increase this.
    vf_clip_param: 10.0
    # If specified clip the global norm of gradients by this amount.
    grad_clip: null
    # Target value for KL divergence.
    kl_target: 0.01
    # Whether to rollout complete_episodes or truncate_episodes.
    batch_mode: complete_episodes
    # Which observation filter to apply to the observation.
    observation_filter: NoFilter #MeanStdFilter
