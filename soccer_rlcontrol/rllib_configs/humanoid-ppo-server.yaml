humanoid-ppo:
    env: gym_soccerbot:walk-forward-norm-v1
    run: PPO
    checkpoint_freq: 20
    max_failures: 10
    checkpoint_at_end: true
    local_dir: ./PPO-walk-forward-norm-v1/
    restore: /home/robosoccer/shah_ws/PPO-walk-forward-norm-v1/humanoid-ppo/PPO_gym_soccerbot:walk-forward-norm-v1_0d4b8_00000_0_2021-04-10_17-52-32/checkpoint_740/checkpoint-740
    stop:
        timesteps_total: 2000000000
    config:
        # Works for both torch and tf.
        framework: torch
        num_workers: 32
        num_cpus_per_worker: 0
        num_envs_per_worker: 4
        horizon: 2048
        num_gpus: 1 # 0.0001
        # num_gpus_per_worker: 0.03
        gamma: 0.95
        # {"OMP_NUM_THREADS": "16"}
        extra_python_environs_for_driver:
            OMP_NUM_THREADS: 32
        env_config:
            env_name: gym_soccerbot:walk-forward-v3
            # dtype: np.float32

        # Should use a critic as a baseline (otherwise don't use value baseline;
        # required for using GAE).
        use_critic: true
        # If true use the Generalized Advantage Estimator (GAE)
        # with a value function see https://arxiv.org/pdf/1506.02438.pdf.
        use_gae: true
        # The GAE (lambda) parameter.
        lambda: 0.99
        # Initial coefficient for KL divergence.
        kl_coeff: 0.2
        # Size of batches collected from each worker.
        rollout_fragment_length: 8192
        # Number of timesteps collected for each SGD round. This defines the size
        # of each SGD epoch.
        train_batch_size: 262144 #4096*32*4 # 4000
        # Total SGD batch size across all devices for SGD. This defines the
        # minibatch size within each epoch.
        sgd_minibatch_size: 8192 #4096*32*4 # 128
        # Whether to shuffle sequences in the batch when training (recommended).
        shuffle_sequences: true
        # Number of SGD iterations in each outer loop (i.e. number of epochs to
        # execute per train batch).
        num_sgd_iter: 10
        # Stepsize of SGD.
        lr: 0.001
        lr_schedule: null
                #    - - !!float 3e-3
                #- !!float 1e-5
                #- !!float 0
        # Learning rate schedule.
        # lr_schedule: null
        # Coefficient of the value function loss. IMPORTANT: you must tune this if
        # you set vf_share_layers=True inside your model's config.
        vf_loss_coeff: 1.0
        model:
            fcnet_hiddens: [64, 64]
            fcnet_activation: tanh
            free_log_std: false
            # Share layers for value function. If you set this to True it's
            # important to tune vf_loss_coeff.
            vf_share_layers: false
        # Coefficient of the entropy regularizer.
        entropy_coeff: 0.001
        # Decay schedule for the entropy regularizer.
        entropy_coeff_schedule: null
        #    - - !!float 1e-2
        #      - !!float 1e-3
        #      - !!float 0
        # PPO clip parameter.
        clip_param: 0.4
        # Clip param for the value function. Note that this is sensitive to the
        # scale of the rewards. If your expected V is large increase this.
        vf_clip_param: 10.0
        # If specified clip the global norm of gradients by this amount.
        grad_clip: null
        # Target value for KL divergence.
        kl_target: 0.01
        # Whether to rollout complete_episodes or truncate_episodes.
        batch_mode: complete_episodes
        # Which observation filter to apply to the observation.
        observation_filter: NoFilter #MeanStdFilter
