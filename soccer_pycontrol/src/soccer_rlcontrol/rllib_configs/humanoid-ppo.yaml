humanoid-ppo:
    env: gym_soccerbot:walk-forward-norm-v1
    run: PPO
    checkpoint_freq: 20
    max_failures: 10
    checkpoint_at_end: true
    local_dir: ./results/
    stop:
        timesteps_total: 10000000
    config:
        # Works for both torch and tf.
        framework: tfe
        # eager: true
        # eager_tracing: true
        num_workers: 12
        num_cpus_per_worker: 0
        num_envs_per_worker: 1
        horizon: 2048
        num_gpus: 1 # Set to 1 for gpu acceleration
        env_config:
            env_name: gym_soccerbot:walk-forward-v3

        # Should use a critic as a baseline (otherwise don't use value baseline;
        # required for using GAE).
        use_critic: True
        # If true use the Generalized Advantage Estimator (GAE)
        # with a value function see https://arxiv.org/pdf/1506.02438.pdf.
        use_gae: True
        # The GAE (lambda) parameter.
        lambda: 1.0
        # Initial coefficient for KL divergence.
        kl_coeff: 0.2
        # Size of batches collected from each worker.
        rollout_fragment_length: 2048
        # Number of timesteps collected for each SGD round. This defines the size
        # of each SGD epoch.
        train_batch_size: 24576 #8192 #4096*4 # 4000
        # Total SGD batch size across all devices for SGD. This defines the
        # minibatch size within each epoch.
        sgd_minibatch_size: 2048 #4096*2 # 128
        # Whether to shuffle sequences in the batch when training (recommended).
        shuffle_sequences: True
        # Number of SGD iterations in each outer loop (i.e. number of epochs to
        # execute per train batch).
        num_sgd_iter: 10
        # Stepsize of SGD.
        lr: !!float 5e-5
        # Learning rate schedule.
        lr_schedule: null
        # Coefficient of the value function loss. IMPORTANT: you must tune this if
        # you set vf_share_layers=True inside your model's config.
        vf_loss_coeff: 1.0
        model:
            fcnet_hiddens: [64, 64]
            fcnet_activation: tanh
            free_log_std: true
            # Share layers for value function. If you set this to True it's
            # important to tune vf_loss_coeff.
            vf_share_layers: False
        # Coefficient of the entropy regularizer.
        entropy_coeff: 0.001
        # Decay schedule for the entropy regularizer.
        entropy_coeff_schedule: null
        # PPO clip parameter.
        clip_param: 0.3
        # Clip param for the value function. Note that this is sensitive to the
        # scale of the rewards. If your expected V is large increase this.
        vf_clip_param: 10.0
        # If specified clip the global norm of gradients by this amount.
        grad_clip: null
        # Target value for KL divergence.
        kl_target: 0.01
        # Whether to rollout complete_episodes or truncate_episodes.
        batch_mode: complete_episodes
        # Which observation filter to apply to the observation.
        observation_filter: NoFilter
